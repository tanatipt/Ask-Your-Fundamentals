# Ask Your Fundamentals

This project implements an end-to-end Retrieval-Augmented Generation (RAG) system designed to process 10-K filings and annual report PDFs of public companies. The system enables analytical question answering about business performance based solely on information extracted from the uploaded documents. It consists of two main components:

1. Index Ingestion Pipeline – Takes uploaded annual report PDFs as input and generates a vector store and keyword store for efficient retrieval.
1. RAG Pipeline – Utilizes these data stores to retrieve relevant information and generate accurate, context-aware answers to user queries, using only the content from the uploaded PDFs.

## Project Structure

When uploading PDFs of annual reports for a public company, we make the following assumptions about the folder structure: each PDF report must be organized by the company’s ticker symbol (e.g., AAPL, AMZN), and each PDF file should be named according to the corresponding reporting year (e.g., 2023.pdf). See the example below for the expected directory layout:

```bash
/src/
└── evaluation_pipeline.py          # Evaluation script to evaluate the retrieval and end-to-end performance of the RAG pipeline.
└── mapper.py                       # Returns the appropriate class to instantiate depending on the arguments passed.
└── qa_creator.py                   # Script to sample questions from the T^2-RAGBench and create the dev set to evaluate our RAG pipeline.
└── utils.py                        # Utility tools and functions used by RAG pipeline evaluation.
└── index_ingestion/               
    └── marker_parser.py            # Defines the parser class that uses the Marker library to convert PDF files into a Markdown format
    └── markdown_chunker.py         # Defines the chunker class that uses Markdown header splitter to chunk parsed document into smaller chunks.
    └── ingestion_main.py           # Defines the index ingestion class that combines the logic of the parser and the chunker.
    └── utils.py                    # Utility tools and functions used by index ingestion pipeline.
└── rag_architecture/              
    └── graph_constructor.py        # Constructs the RAG pipeline and connect different components in the pipeline together
    └── components/
        └── extract_answer.py       # Node for extracting the answer and citation generated by the LLM.
        └── generate_answer.py      # Node that uses tool-calling agent to perform mathematical calculations or generate final answer.
        └── generate_response.py    # Node for generating answer to user queries that are not relevant to the topics in the RAG system.
        └── retrieve_content.py     # Retrieve relevant chunks from the vectorstore and keyword store.
        └── rewrite_query.py        # Node to contextualise the user query based on the conversation history and determine intention of user.
        └── schemas.py              # Defines all the Pydantic schemas used in the RAG pipeline.
        └── utils.py                # Utility tools and functions used by the RAG pipeline.

/data/
└── reports/                        # Contains the uploaded PDF reports organised in the expected layout for the parser to function correctly.
    └── GS/
        └── 2012.pdf
        └── 2013.pdf
    └── JPM/
        └── 2013.pdf
        └── 2014.pdf

/config/
└── settings.yaml                   # Configuration file specifying the configuration for the index ingestion and RAG pipeline.


```

## Evaluation Dataset

To evaluate our RAG pipeline, we constructed an evaluation dataset based on the T^2-RAGBench benchmark. The T^2-RAGBench contains 32,908 question–context–answer triples designed to assess RAG methods using real-world financial data. After conducting EDA on the benchmark, we identified Goldman Sachs (GS) as the company with the highest number of associated questions and selected it as the focus of our evaluation.

### Dataset Composition
To construct our dataset, we collected publicly available annual report PDFs for Goldman Sachs (GS) across multiple years (**2012-2014**, **2016-2017**) and curated a set of 60 evaluation questions from the benchmark, structured as follows:
- Answerable Questions (70% – 42 questions): These questions focus on GS’s performance and can be directly answered using information contained in the collected annual reports.
- Unanswerable Questions (15% – 9 questions): These questions also concern GS but cannot be answered because the necessary information is not present in the available reports. For example, a question about GS’s profit in 2007 would be unanswerable since we only collected reports between **2012-2014** and **2016-2017**
- Negative Questions (15% – 9 questions): These questions are unrelated to GS, such as those concerning other financial institutions like JP Morgan or BlackRock.

### Sample Questions

| Question Type | Question | 
| --------------| ---------|
| Answerable | What were the average incentive fees in millions for the three-year period from 2010 to 2012, as reflected in the operating results of Goldman Sachs' investment management segment?|
| Answerable | What percentage of the 2011 net interest income did the $3.88 billion net interest income in 2012 represent for Goldman Sachs?|
| Answerable | What percentage of Goldman Sachs' cash and cash equivalents at the end of 2013 was generated from operating activities, considering the company's complex cash flows as a global financial institution?|
| Unanswerable | What percentage of Goldman Sachs' total net revenues in the Institutional Client Services segment for the year ended December 2015 was attributed to Fixed Income, Currency, and Commodities client execution?|
| Unanswerable | What was the fair value of retained interests held by Goldman Sachs as of December 2018?|
| Negative | What were the net sales for North American Printing Papers in 2006?|
| Negative | What is the cumulative value of income from continuing operations and loss from discontinued operations, net of tax, per diluted share for the Charles Schwab Corporation in 2008?|

## Methodology
### Index Ingestion Pipeline

The diagram below illustrates the index ingestion pipeline. Starting with a PDF file, we first use the Marker parser library to convert it into a Markdown file. This intermediate Markdown file is then saved to avoid the need for re-parsing later, which can be time-consuming. Next, we apply a Markdown header splitter to divide the document into smaller, more manageable chunks. These chunks are then embedded using the  `gemini-001-embedding` model and stored in a Chroma vectorstore. In parallel, they are also further preprocessed to build a BM25 keyword index for efficient keyword-based retrieval.

<img src='resources/index-ingestion.png'>

#### Marker Parser 

**Tool Choice** We utilized the Marker parser library to convert uploaded PDF files into Markdown format. While we considered other options such as PyMuPDF and Llamaparse, Marker proved to be superior in several ways. Compared to PyMuPDF, Marker generated more structured and cleaner markdown outputs, as it leverages computer vision models to better interpret the layout and formatting of PDF documents. In contrast to Llamaparse, Marker is significantly faster and more cost-effective because it performs parsing locally without relying on external API calls.

For each PDF file, we begin the parsing process begins with page classification where we identify whether each page in the PDF is primarily text-based or image-based based on image-to-text area ratio and presence of searchable text content. Text pages are converted directly into Markdown by Marker without using OCR, whereas image pages undergo OCR extraction to retrieve their text content. Importantly, no large language models (LLMs) are involved in the parsing process—only computer vision models are used. To minimize noise in the resulting Markdown documents, we configured Marker to automatically exclude header and footer sections, which often contain irrelevant information such as page numbers or disclaimers and are typically repeated across multiple pages.

Since Marker runs locally on GPU resources, parsing is conducted page by page to efficiently manage memory usage. Additionally, for every page, we collect and store metadata returned by Marker, such as the presence of images or tables, along with the corresponding page number. These metadata elements are attached to each page to facilitate downstream processing and analysis.



#### Markdown Chunker

After extracting the Markdown files from the PDFs, we implemented and used a custom chunker to split the documents into smaller, more manageable chunks so that it can be indexed in a vector and keyword store. The process involves the following steps:

1. **Cleaning the Markdown text**: We first remove unnecessary noise from the Markdown files. This includes replacing image tags and hyperlinks with their alt text, stripping out HTML and standalone URLs, and normalizing whitespace to create a cleaner, more consistent text.

2. **Header-based chunking**: Financial reports are generally well-structured with clear sections and headers. To preserve this natural structure, we used LangChain’s **MarkdownHeaderTextSplitter** to split the documents according to their header hierarchy. This approach ensures that all content under the same header remains together in a single chunk, preventing related information from being fragmented across multiple chunks.

3. **Merging and overlapping chunks**: After initial splitting, some chunks can be very small. We merged such chunks to achieve a more balanced chunk size, reducing the total number of chunks. Additionally, we introduced overlaps between consecutive chunks to maintain context continuity, ensuring that important information spanning sections is not lost.

#### Chroma Vectorstore


#### BM25 Keywordstore


### RAG Pipeline

#### Query Contextualisation

#### Document Retrieval

#### ReACT Agent

#### Handling Unrelated Questions

## Evaluation

## Installation & Project Setup